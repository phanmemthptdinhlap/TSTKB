{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bf37937",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa60328",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "class vitri_Encoding(nn.Module):\n",
    "    def __init__(self, kichthuoc_vector,dodaitoidachuoi):\n",
    "        super(vitri_Encoding, self).__init__()\n",
    "        vt_en=torch.zeros(dodaitoidachuoi, kichthuoc_vector)\n",
    "        vt=torch.arange(0, dodaitoidachuoi, dtype=torch.float).unsqueeze(1)\n",
    "        hschia=torch.exp(torch.arange(0, kichthuoc_vector, 2).float() * -(math.log(10000.0) / kichthuoc_vector))\n",
    "        vt_en[:, 0::2] = torch.sin(vt * hschia)\n",
    "        vt_en[:, 1::2] = torch.cos(vt * hschia)\n",
    "        vt_en = vt_en.unsqueeze(0)\n",
    "        self.register_buffer('vt_en', vt_en)\n",
    "    def forward(self, x):\n",
    "        return x + self.vt_en[:, :x.size(1), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc96b039",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "class khoi_tranformer_Encoder(nn.Module):\n",
    "    def __init__ (self, kichthuoc_vector,soluong_dauvao,kichthuoc_tang,tile_dropout=0.1):\n",
    "        super(khoi_tranformer_Encoder, self).__init__()\n",
    "        self.tang_dauvao_chu_y=nn.MultiheadAttention(\n",
    "            embed_dim=kichthuoc_vector,\n",
    "            num_heads=soluong_dauvao,\n",
    "            dropout=tile_dropout\n",
    "        )\n",
    "        self.mang_motchieu=nn.Sequential(\n",
    "            nn.Linear(kichthuoc_vector, kichthuoc_tang),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(kichthuoc_tang, kichthuoc_vector)\n",
    "        )\n",
    "        self.tangchuanhoa1=nn.LayerNorm(kichthuoc_vector, eps=1e-6)\n",
    "        self.tangchuanhoa2=nn.LayerNorm(kichthuoc_vector, eps=1e-6)\n",
    "        self.tang_dropout1=nn.Dropout(tile_dropout)\n",
    "        self.tang_dropout2=nn.Dropout(tile_dropout)\n",
    "    def forward(self, x,mask=None):\n",
    "        chu_y, _ = self.tang_dauvao_chu_y(x, x, x, attn_mask=mask)\n",
    "        chu_y=self.tang_dropout1(chu_y)\n",
    "        dulieura1= x + chu_y\n",
    "        dulieura1=self.tangchuanhoa1(dulieura1)\n",
    "        dulieura2=self.mang_motchieu(dulieura1)\n",
    "        dulieura2=self.tang_dropout2(dulieura2)\n",
    "        dulieura2 = dulieura1 + dulieura2\n",
    "        dulieura2=self.tangchuanhoa2(dulieura2)\n",
    "        return dulieura2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e364faa6",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "class lop_tranformer_Encoder(nn.Module):\n",
    "    def __init__(self, soluong_lop, kichthuoc_vector, soluong_dauvao,\n",
    "                kichthuoc_tang, soluong_tuvung,\n",
    "                dodaichuoi_toida,tile_dropout=0.1):\n",
    "        \n",
    "        super(lop_tranformer_Encoder, self).__init__()\n",
    "        self.kichthuoc_vector = kichthuoc_vector\n",
    "        self.tang_nhung=nn.Embedding(soluong_tuvung, kichthuoc_vector)\n",
    "        self.mahoa_vitri = vitri_Encoding(kichthuoc_vector, dodaichuoi_toida)\n",
    "        self.danhsach_khoi_xuly= nn.ModuleList([\n",
    "            khoi_tranformer_Encoder(\n",
    "                kichthuoc_vector, soluong_dauvao, \n",
    "                kichthuoc_tang, tile_dropout) for _ in range(soluong_lop)\n",
    "            ])\n",
    "        self.lop_dropout = nn.Dropout(tile_dropout)\n",
    "    def forward(self, x, mask=None):\n",
    "        x= self.tang_nhung(x)* math.sqrt(self.kichthuoc_vector)\n",
    "        x = self.mahoa_vitri(x)\n",
    "        x = self.lop_dropout(x)\n",
    "        for khoi_xuly in self.danhsach_khoi_xuly:\n",
    "            x = khoi_xuly(x, mask)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e19b799",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def tao_mohinh_tranformer_Encoder(soluong_lop, kichthuoc_vector, soluong_dauvao,\n",
    "                            kichthuoc_tang, soluong_tuvung,\n",
    "                            dodaichuoi_toida, tile_dropout=0.1):\n",
    "    return lop_tranformer_Encoder(\n",
    "        soluong_lop, kichthuoc_vector, soluong_dauvao,\n",
    "        kichthuoc_tang, soluong_tuvung,\n",
    "        dodaichuoi_toida, tile_dropout\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f90da7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lop_tranformer_Encoder(\n",
       "  (tang_nhung): Embedding(1000, 64)\n",
       "  (mahoa_vitri): vitri_Encoding()\n",
       "  (danhsach_khoi_xuly): ModuleList(\n",
       "    (0-1): 2 x khoi_tranformer_Encoder(\n",
       "      (tang_dauvao_chu_y): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
       "      )\n",
       "      (mang_motchieu): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=128, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=128, out_features=64, bias=True)\n",
       "      )\n",
       "      (tangchuanhoa1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
       "      (tangchuanhoa2): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
       "      (tang_dropout1): Dropout(p=0.1, inplace=False)\n",
       "      (tang_dropout2): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (lop_dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "#Tham số mô hình\n",
    "soluong_lop = 2\n",
    "kichthuoc_vector = 64\n",
    "soluong_dauvao = 4\n",
    "kichthuoc_tang = 128\n",
    "soluong_tuvung = 1000\n",
    "dodaichuoi_toida = 100 \n",
    "tile_dropout = 0.1\n",
    "# Tạo mô hình\n",
    "mohinh = tao_mohinh_tranformer_Encoder(\n",
    "    soluong_lop, kichthuoc_vector, soluong_dauvao,\n",
    "    kichthuoc_tang, soluong_tuvung,\n",
    "    dodaichuoi_toida, tile_dropout\n",
    ")\n",
    "mohinh.eval()  # Chuyển mô hình sang chế độ đánh giá"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf960d0",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'khoi_tranformer_Encoder' object has no attribute 'tang_dopout1'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m padding_mask = padding_mask.unsqueeze(\u001b[32m1\u001b[39m).unsqueeze(\u001b[32m2\u001b[39m)\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m     output = \u001b[43mmohinh\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mKích thước đầu ra:\u001b[39m\u001b[33m\"\u001b[39m, output.shape)\n\u001b[32m      9\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mĐầu ra mẫu:\u001b[39m\u001b[33m\"\u001b[39m, output[\u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m, :\u001b[32m5\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/PyProjects/.Env/lib64/python3.13/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/PyProjects/.Env/lib64/python3.13/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 21\u001b[39m, in \u001b[36mlop_tranformer_Encoder.forward\u001b[39m\u001b[34m(self, x, mask)\u001b[39m\n\u001b[32m     19\u001b[39m x = \u001b[38;5;28mself\u001b[39m.lop_dropout(x)\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m khoi_xuly \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.danhsach_khoi_xuly:\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m     x = \u001b[43mkhoi_xuly\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/PyProjects/.Env/lib64/python3.13/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/PyProjects/.Env/lib64/python3.13/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 20\u001b[39m, in \u001b[36mkhoi_tranformer_Encoder.forward\u001b[39m\u001b[34m(self, x, mask)\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x,mask=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m     19\u001b[39m     chu_y, _ = \u001b[38;5;28mself\u001b[39m.tang_dauvao_chu_y(x, x, x, attn_mask=mask)\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m     chu_y=\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtang_dopout1\u001b[49m(chu_y)\n\u001b[32m     21\u001b[39m     dulieura1= x + chu_y\n\u001b[32m     22\u001b[39m     dulieura1=\u001b[38;5;28mself\u001b[39m.tangchuanhoa1(dulieura1)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/PyProjects/.Env/lib64/python3.13/site-packages/torch/nn/modules/module.py:1940\u001b[39m, in \u001b[36mModule.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   1938\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[32m   1939\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[32m-> \u001b[39m\u001b[32m1940\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[32m   1941\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m object has no attribute \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1942\u001b[39m )\n",
      "\u001b[31mAttributeError\u001b[39m: 'khoi_tranformer_Encoder' object has no attribute 'tang_dopout1'"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "sample_data = torch.tensor([[0, 1, 4, 3, 2, 3, 2, 3, 5, 4, 5, 6]], dtype=torch.long)\n",
    "\n",
    "padding_mask = (sample_data == 0).float() * -1e9\n",
    "padding_mask = padding_mask.unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = mohinh(sample_data, mask=None)\n",
    "    print(\"Kích thước đầu ra:\", output.shape)\n",
    "    print(\"Đầu ra mẫu:\", output[0, 0, :5])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".Env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
