{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bf37937",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7aa60328",
   "metadata": {},
   "outputs": [],
   "source": [
    "class vitri_Encoding(nn.Module):\n",
    "    def __init__(self, kichthuoc_vector,dodaitoidachuoi):\n",
    "        super(vitri_Encoding, self).__init__()\n",
    "        vt_en=torch.zeros(dodaitoidachuoi, kichthuoc_vector)\n",
    "        vt=torch.arange(0, dodaitoidachuoi, dtype=torch.float).unsqueeze(1)\n",
    "        hschia=torch.exp(torch.arange(0, kichthuoc_vector, 2).float() * -(math.log(10000.0) / kichthuoc_vector))\n",
    "        vt_en[:, 0::2] = torch.sin(vt * hschia)\n",
    "        vt_en[:, 1::2] = torch.cos(vt * hschia)\n",
    "        vt_en = vt_en.unsqueeze(0)\n",
    "        self.register_buffer('vt_en', vt_en)\n",
    "    def forward(self, x):\n",
    "        return x + self.vt_en[:, :x.size(1), :]"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 3,
   "id": "bc96b039",
   "metadata": {},
   "outputs": [],
=======
   "execution_count": null,
   "id": "3af2c1c3",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torch.nn' has no attribute 'Moudle'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmang_tuyentinh\u001b[39;00m(\u001b[43mnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mMoudle\u001b[49m):\n\u001b[32m      2\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, kichthuoc_vetor, kichthuoc_tang):\n\u001b[32m      3\u001b[39m         \u001b[38;5;28msuper\u001b[39m(mang_tuyentinh,\u001b[38;5;28mself\u001b[39m).\u001b[34m__init__\u001b[39m()\n",
      "\u001b[31mAttributeError\u001b[39m: module 'torch.nn' has no attribute 'Moudle'"
     ]
    }
   ],
>>>>>>> origin/main
   "source": [
    "class mang_tuyentinh(nn.Module):\n",
    "    def __init__(self, kichthuoc_vetor, kichthuoc_tang):\n",
    "        super(mang_tuyentinh,self).__init__()\n",
    "        self.lop1=nn.Linear(kichthuoc_vetor,kichthuoc_tang)\n",
    "        self.lop2=nn.Linear(kichthuoc_tang,kichthuoc_vetor)\n",
    "    def forward(self,x):\n",
    "        x=self.lop1(x)\n",
    "        x=nn.ReLU(x)\n",
    "        x=self.lop2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc96b039",
   "metadata": {},
   "outputs": [],
   "source": [
    "class khoi_tranformer_Encoder(nn.Module):\n",
    "    def __init__ (self, kichthuoc_vector,soluong_dauvao,kichthuoc_tang,tile_dropout=0.1):\n",
    "        super(khoi_tranformer_Encoder, self).__init__()\n",
    "        self.tang_dauvao_chu_y=nn.MultiheadAttention(\n",
    "            embed_dim=kichthuoc_vector,\n",
    "            num_heads=soluong_dauvao,\n",
    "            dropout=tile_dropout\n",
    "        )\n",
    "        self.mang_motchieu=mang_tuyentinh(kichthuoc_vector,kichthuoc_tang)\n",
    "        self.tangchuanhoa1=nn.LayerNorm(kichthuoc_vector, eps=1e-6)\n",
    "        self.tangchuanhoa2=nn.LayerNorm(kichthuoc_vector, eps=1e-6)\n",
    "        self.tang_dropout1=nn.Dropout(tile_dropout)\n",
    "        self.tang_dropout2=nn.Dropout(tile_dropout)\n",
    "    def forward(self, x,mask=None):\n",
    "        chu_y, _ = self.tang_dauvao_chu_y(x, x, x, attn_mask=mask)\n",
    "        chu_y=self.tang_dropout1(chu_y)\n",
    "        dulieura1= x + chu_y\n",
    "        dulieura1=self.tangchuanhoa1(dulieura1)\n",
    "        dulieura2=self.mang_motchieu(dulieura1)\n",
    "        dulieura2=self.tang_dropout2(dulieura2)\n",
    "        dulieura2 = dulieura1 + dulieura2\n",
    "        dulieura2=self.tangchuanhoa2(dulieura2)\n",
    "        return dulieura2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e364faa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class mohinh_tranformer_Encoder(nn.Module):\n",
    "    def __init__(self, soluong_lop, kichthuoc_vector, soluong_dauvao,\n",
    "                kichthuoc_tang, soluong_tuvung,\n",
    "                dodaichuoi_toida,tile_dropout=0.1):\n",
    "        \n",
    "        super(mohinh_tranformer_Encoder, self).__init__()\n",
    "        self.kichthuoc_vector = kichthuoc_vector\n",
    "        self.tang_nhung=nn.Embedding(soluong_tuvung, kichthuoc_vector)\n",
    "        self.mahoa_vitri = vitri_Encoding(kichthuoc_vector, dodaichuoi_toida)\n",
    "        self.danhsach_khoi_xuly= nn.ModuleList([\n",
    "            khoi_tranformer_Encoder(\n",
    "                kichthuoc_vector, soluong_dauvao, \n",
    "                kichthuoc_tang, tile_dropout) for _ in range(soluong_lop)\n",
    "            ])\n",
    "        self.lop_dropout = nn.Dropout(tile_dropout)\n",
    "    def forward(self, x, mask=None):\n",
    "        x= self.tang_nhung(x)* math.sqrt(self.kichthuoc_vector)\n",
    "        x = self.mahoa_vitri(x)\n",
    "        x = self.lop_dropout(x)\n",
    "        for khoi_xuly in self.danhsach_khoi_xuly:\n",
    "            x = khoi_xuly(x, mask)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e19b799",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tao_mohinh_tranformer_Encoder(soluong_lop, kichthuoc_vector, soluong_dauvao,\n",
    "                            kichthuoc_tang, soluong_tuvung,\n",
    "                            dodaichuoi_toida, tile_dropout=0.1):\n",
    "    return mohinh_tranformer_Encoder(\n",
    "        soluong_lop, kichthuoc_vector, soluong_dauvao,\n",
    "        kichthuoc_tang, soluong_tuvung,\n",
    "        dodaichuoi_toida, tile_dropout\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 6,
=======
   "execution_count": null,
   "id": "433aecd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class lop_tranformer_Decoder(nn.Module):\n",
    "    def __init__(self,kichthuoc_vertor, soluong_dauvao, kichthuoc_tang,tile_dropout=0.1):\n",
    "        super(lop_tranformer_Decoder,self).__init__()\n",
    "        self.tangvaochuy=nn.MultiheadAttention(kichthuoc_vertor,soluong_dauvao)\n",
    "        self.mangtuyentinh=mang_tuyentinh(kichthuoc_vertor,kichthuoc_tang)\n",
    "        self.chuanhoa1=nn.LayerNorm(kichthuoc_vertor)\n",
    "        self.chuanhoa2=nn.LayerNorm(kichthuoc_vertor)\n",
    "        self.dropout=nn.Dropout(tile_dropout)\n",
    "    def forward(self, x, mask):\n",
    "        att_out=self.tangvaochuy(x,x,x,mask)\n",
    "        x=self.chuanhoa1(x+self.dropout(att_out))\n",
    "        ffn_out=self.mangtuyentinh(x)\n",
    "        x=self.chuanhoa2(x+self.dropout(ffn_out))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ddb58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class mohinh_tranformer_Decoder(nn.Modul):\n",
    "    def __init__(self,kichthuoc_vertor, soluong_tuvung, \n",
    "                 soluong_dauvao, solop, kichthuoc_vung,\n",
    "                 chuoidaitoida,tiledropout=0.1):\n",
    "        super(mohinh_tranformer_Decoder,self).__init__()\n",
    "        self.nhung=nn.Embedding(soluong_tuvung,kichthuoc_vertor)\n",
    "        self.mahoavitri=vitri_Encoding(kichthuoc_vertor,chuoidaitoida)\n",
    "        self.khoi_xuly=nn.ModuleList([\n",
    "            lop_tranformer_Decoder(kichthuoc_vertor,\n",
    "                    soluong_dauvao,kichthuoc_vung,\n",
    "                    tiledropout) for _ in range(solop)\n",
    "        ])\n",
    "        self.loptuyentinh=nn.Linear(kichthuoc_vertor,soluong_tuvung)\n",
    "        self.dropout=nn.Dropout(tiledropout)\n",
    "    def generate_mask(self, dodaichuoi):\n",
    "        mask=torch.triu(torch.ones(dodaichuoi,dodaichuoi),diagonal=1).bool()\n",
    "        mask=mask.unsqueeze(0).unsqueeze(0)\n",
    "        return mask\n",
    "    def forward(self,x):\n",
    "        seq_len=x.size(1)\n",
    "        mask=self.generate_mask(seq_len).to(x.device)\n",
    "        x=self.nhung(x)\n",
    "        x=self.mahoavitri(x)\n",
    "        x=self.dropout(x)\n",
    "        for lop in self.khoi_xuly:\n",
    "            x=lop(x,mask)\n",
    "        return self.loptuyentinh(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
>>>>>>> origin/main
   "id": "54f90da7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lop_tranformer_Encoder(\n",
       "  (tang_nhung): Embedding(1000, 64)\n",
       "  (mahoa_vitri): vitri_Encoding()\n",
       "  (danhsach_khoi_xuly): ModuleList(\n",
       "    (0-1): 2 x khoi_tranformer_Encoder(\n",
       "      (tang_dauvao_chu_y): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
       "      )\n",
       "      (mang_motchieu): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=128, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=128, out_features=64, bias=True)\n",
       "      )\n",
       "      (tangchuanhoa1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
       "      (tangchuanhoa2): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
       "      (tang_dropout1): Dropout(p=0.1, inplace=False)\n",
       "      (tang_dropout2): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (lop_dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#Tham số mô hình\n",
    "soluong_lop = 2\n",
    "kichthuoc_vector = 64\n",
    "soluong_dauvao = 4\n",
    "kichthuoc_tang = 128\n",
    "soluong_tuvung = 1000\n",
    "dodaichuoi_toida = 100 \n",
    "tile_dropout = 0.1\n",
    "# Tạo mô hình\n",
    "mohinh = tao_mohinh_tranformer_Encoder(\n",
    "    soluong_lop, kichthuoc_vector, soluong_dauvao,\n",
    "    kichthuoc_tang, soluong_tuvung,\n",
    "    dodaichuoi_toida, tile_dropout\n",
    ")\n",
    "mohinh.eval()  # Chuyển mô hình sang chế độ đánh giá"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9bf960d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kích thước đầu ra: torch.Size([1, 12, 64])\n",
<<<<<<< HEAD
      "Đầu ra mẫu: tensor([-0.7253,  0.2042, -1.4659,  1.6815, -0.0865])\n"
=======
      "Đầu ra mẫu: tensor([ 1.7824,  0.1652, -1.0553, -0.7577,  0.1202])\n"
>>>>>>> origin/main
     ]
    }
   ],
   "source": [
    "sample_data = torch.tensor([[0, 1, 4, 3, 2, 3, 2, 3, 5, 4, 5, 6]], dtype=torch.long)\n",
    "\n",
    "padding_mask = (sample_data == 0).float() * -1e9\n",
    "padding_mask = padding_mask.unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = mohinh(sample_data, mask=None)\n",
    "    print(\"Kích thước đầu ra:\", output.shape)\n",
    "    print(\"Đầu ra mẫu:\", output[0, 0, :5])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
